More Glue

Credit: This lab is based on the Serverless Data Lake Immersion Day lab by @akirmak.

Be sure to have completed Part2 as this will use the amazon_reviews_us_Kitchen_v1_00.tsv.gz dataset you uploaded to your S3 bucket.

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores. 

Why AWS Glue?
Given that the alternative would be to run distributed systems Spark yourself, you can stay on the shoulders of the giant AWS infrastructure and let them manage it for you.



Navigate to Glue console

Navigate to your reviews table

Note that it is correctly identified as CSV.

The transform pipeline is the next piece of the puzzle. It defines the necessary steps to transform partitioned raw JSON data from the AWS S3 raw data bucket to partitioned parquet data in the AWS S3 processed data bucket.

In this lab, you will create a job to perform basic tranformation on the source data: You will 
•	rename two fields, 
•	drop one field
•	convert the raw data into a compressed columnar format (Parquet) to another S3 bucket.

Glue concepts used in the lab: 

•	ETL Operations: Using the metadata in the Data Catalog, AWS Glue can autogenerate Scala or PySpark (the Python API for Apache Spark) scripts with AWS Glue extensions that you can use and modify to perform various ETL operations. For example, you can extract, clean, and transform raw data, and then store the result in a different repository, where it can be queried and analyzed. Such a script might convert a CSV file into a relational form and save it in Amazon Redshift.

•	Jobs- The AWS Glue Jobs system provides managed infrastructure to orchestrate your ETL workflow. You can create jobs in AWS Glue that automate the scripts you use to extract, transform, and transfer data to different locations. Jobs can be scheduled and chained, or they can be triggered by events such as the arrival of new data.
o	AWS Glue runs your ETL jobs in an Apache Spark serverless environment. 
o	AWS Glue can generate a script to transform your data. Or, you can provide the script in the AWS Glue console or API.
o	You can run your job on demand, or you can set it up to start when a specified trigger occurs. The trigger can be a time-based schedule or an event.
o	When your job runs, a script extracts data from your data source, transforms the data, and loads it to your data target. The script runs in an Apache Spark serverless environment in AWS Glue.


1.	Let’s assume, we’d like to have a central repository of our ETL scrips fort his Project. Instead of using the default path of Glue, first create a folder under your bucket “lf-datalake-[number]” called  “scripts-etl”
o	Open S3 service in a separate tab (don’t close the current Glue tab, we’ll need to continue
o	G oto bucket “<your-initials>-tame-bda-immersion” 
o	Create a folder called  “scripts-etl” underneath. 

* Create a folder called "glue-sparkui-history"


2.	G oto Glue console and Create a glue job with the configuration below by selecting Glue -> ETL -> Jobs “Add Job”. Enter job details like below:
o	Job Properties Stage:
o	Job name: csv2parquet
o	IAM Role: 
	Choose Lab-IntroDataLake-Glue
Type: Spark
Glue version: Spark 2.4, Python 3
o	This job runs:
	Select “A proposed script generated by AWS Glue” 

o	Script name: leave at as default
o	S3 path for script: 
	browse for the scripts-etl folder in your lf-datalake bucket
o	S3 path for temp files: Do not change. <glue provides a default value>
o	Open Advanced Properties section:
	job bookmarks:  enable
Open up the Monitoring options section
	job metrics: check the box
sparkui: check the box and select the glue-sparkui-history folder

click next

3.	Data source : Select the reviews table from our Glue DB

click next

choose Change Schema

click next

4.	Data Target : Select Create Tables in your data target.
 select the DS as S3, and type as Parquet. Enter s3://lf-datalake/processed/reviews

 click next




 click save job and edit script

 
Notice: This image (from another Project) provides a graphical explanation of the several sections in the code. If you don’t have previous experience with PySpark, Data Frames or Python development, don’t worry. You won’t be making any coding here. The code simply implements the field mapping transformations you specified in the GUI (a field is dropped out, other fields have been renamed etc.). The data engineering team will likely write code for transformations in your data source. Simple transformations can be done from the GUI.
  
 
**** PUT IMAGE FROM WORD DOC HERE ****

Save the job and close the edit script window (click the X in the upper left)





Advanced Data Preparation with Glue Developer Endpoints and Notebook
An ETL job with basic transformations can be created from Glue Console as explained in the previous Lab. Often, the data sources, the data formats and transformation requirements are diverse and complex. In that case, a REPL (read-eval-print-loop) approach, where an interactive programming environment such as a Jypiter Notebook running PySpark or Scala is needed for data preparation. 

Data Engineers will find the Developer Endpoint feature for AWS Glue very useful. They can spin up an endpoint when they are ready to build a pipeline. After trying some data manipulations in a REPL fashion, one can have Glue build an EC2 instance to host Jupyter and build a PySpark script to be saved in S3. When they are done, they just tear it all down; the best part is that if they want to go back and modify the script, they can create a new notebook and even collaborate with other Data Engineers on the same pipelines. It’s almost a source code management system for data scripting.

In this lab, you will create a developer endpoint to connect to the Glue service, and do some iterative development. 

Here are some resources for Glue Developer Endpoints:
https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-prerequisites.html 
https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-notebook-server-considerations.html


Note: This section is relevant for data engineers & software developers.

Lab 2.3.1 Create a Developer Endpoint:

A development endpoint is an environment used to develop and test your AWS Glue scripts. This is useful if you need to do advanced data preparation on your data in the data lake that is beyond the rename field, add/drop field type of transformations you have done in the previous lab.  

1.	Create a Glue Development Endpoint.  Click on Dev Endpoints.  Then click add endpoint.
Use the configuration below:
i.	Properties:
	Endpoint-name: e.g. introdatalake-devEndpoint
	IAM Role: Select the role Lab-IntroDataLake-Glue
	DPU’s: 5 (the default)
Expand monitoring options.  Check the box for Spark UI.  Set the S3 folder to glue-sparkui-history

Click Next

Choose Skip Networking information

Click next

We will use a Sagemaker notebook.  Click Next on the SSH key page.

Click Finish

Take 5minutes or so to provision

When ready, select your endpoint.  Go Actions.. Create Sagemaker notebook.

Notebook name: aws-glue-introdatalake
Choose Create a role: AWSGlueServiceSageMakerNotebookRole-introdatalake

Click Create notebook

This will take about 5 minutes or so to be ready

Click on your new notebook

Then click Open

Create a new notebook by going New... SParkMagic (PySpark)

In the first paragraph, paste this code.

```
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

glueContext = GlueContext(SparkContext.getOrCreate())

```

Run the first paragraph.  This will take a few minutes to complete.  You see the [*] showing that the paragraph is still executing.




In the glue console, click on Jobs.
Then click on csv2parquet
Then click on the Script tab

Starting with the datasource line, select the datasource section and copy to your clipboard

Go back to your new Notebook
Paste in the script in the second paragraph

It should look like this:

Run the second paragraph.

Now copy these lines into the 3rd paragraph

```
print ("Count: ", datasource0.count())

```

Comment out the datasink line.


... copy in some code?

run the first paragraph to make the initial connection




```
from pyspark.sql.functions import concat, to_date, col, lit, udf
from pyspark.sql.types import StringType
from awsglue.dynamicframe import DynamicFrame

## @type: ApplyMapping
## @args: [mapping = [("marketplace", "string", "marketplace", "string"), ("customer_id", "long", "customer_id", "long"), ("review_id", "string", "review_id", "string"), ("product_id", "string", "product_id", "string"), ("product_parent", "long", "product_parent", "long"), ("product_title", "string", "product_title", "string"), ("product_category", "string", "product_category", "string"), ("star_rating", "long", "star_rating", "long"), ("helpful_votes", "long", "helpful_votes", "long"), ("total_votes", "long", "total_votes", "long"), ("vine", "string", "vine", "string"), ("verified_purchase", "string", "verified_purchase", "string"), ("review_headline", "string", "review_headline", "string"), ("review_body", "string", "review_body", "string"), ("review_date", "string", "review_date", "string")], transformation_ctx = "applymapping1"]
## @return: applymapping1
## @inputs: [frame = datasource0]
applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("marketplace", "string", "marketplace", "string"), ("customer_id", "long", "customer_id", "long"), ("review_id", "string", "review_id", "string"), ("product_id", "string", "product_id", "string"), ("product_parent", "long", "product_parent", "long"), ("product_title", "string", "product_title", "string"), ("product_category", "string", "product_category", "string"), ("star_rating", "long", "star_rating", "long"), ("helpful_votes", "long", "helpful_votes", "long"), ("total_votes", "long", "total_votes", "long"), ("vine", "string", "vine", "string"), ("verified_purchase", "string", "verified_purchase", "string"), ("review_headline", "string", "review_headline", "string"), ("review_body", "string", "review_body", "string"), ("review_date", "string", "review_date", "string")], transformation_ctx = "applymapping1")
## @type: ResolveChoice
## @args: [choice = "make_struct", transformation_ctx = "resolvechoice2"]
## @return: resolvechoice2
## @inputs: [frame = applymapping1]
resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = "make_struct", transformation_ctx = "resolvechoice2")
## @type: DropNullFields
## @args: [transformation_ctx = "dropnullfields3"]
## @return: dropnullfields3
## @inputs: [frame = resolvechoice2]
dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = "dropnullfields3")


## print (dropnullfields3.count())
dropnullfields3.printSchema()

## custom Python function
def setYear(review_date):
    if review_date == None:
       return ""
    else:
       return review_date[0:4]
        
addYearDf = dropnullfields3.toDF()
## addYearDf.show()
year_udf = udf(setYear, StringType() )

addYearDf = addYearDf.withColumn("review_year", year_udf("review_date"))
addYearDf.show(5)

countsByYear = addYearDf.groupBy("review_year").count()
countsByYear.show()

addYearDf = addYearDf.filter(addYearDf.review_year.between(2000,2015))

countsByYear = addYearDf.groupBy("review_year").count()
countsByYear.show()

addYear4 = DynamicFrame.fromDF(addYearDf, glueContext, "addYear4")

print ("Done", addYear4.count())

## @type: DataSink
## @args: [connection_type = "s3", connection_options = {"path": "s3://lab-introdatalake-acme/processed_demo/reviews"}, format = "parquet", transformation_ctx = "datasink4"]
## @return: datasink4
## @inputs: [frame = dropnullfields3]
## datasink4 = glueContext.write_dynamic_frame.from_options(frame = addYear4,
##                connection_type = "s3",
##                connection_options = {"path": "s3://lab-introdatalake-acme/processed_demo/reviews", "partitionKeys": ["review_year"]}, 
##                format = "parquet",
##                transformation_ctx = "datasink4")
print ("Done2")
## job.commit()
print ("Done3")

```










* edit script to be partitioned...

https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html

add the

```
, "partitionKeys": ["review_year"]
```

datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = "s3", connection_options = {"path": "s3://lab-introdatalake-acme/processed_demo/reviews", "partitionKeys": ["review_year"]}, format = "parquet", transformation_ctx = "datasink4")

Then add this code...

```
def setMonthNumber(string):
    m = {
        'jan': 1,
        'feb': 2,
        'mar': 3,
        'apr':4,
         'may':5,
         'jun':6,
         'jul':7,
         'aug':8,
         'sep':9,
         'oct':10,
         'nov':11,
         'dec':12
        }
    s = string.strip()[:3].lower()
    try:
        out = m[s]
        return out
    except:
        raise ValueError('Not a month')

def setYear(month):
    if month < 10:
        return 2019
    else:
        return 2018
        
timestampedDf = dropnullfields3.toDF()
month_udf = udf(setMonthNumber, StringType() )
year_udf = udf(setYear, StringType() )

timestampedDf = timestampedDf.withColumn("monthNumber", month_udf("month"))
timestampedDf = timestampedDf.withColumn("year", year_udf("monthNumber"))
timestampedDf = timestampedDf.withColumn("processed-date", to_date(concat(col("year"), lit('-'), col('monthNumber'), lit('-'), col('day'))))

timestamped4 = DynamicFrame.fromDF(timestampedDf, glueContext, "timestamped4")
```



More examples of Glue ETL coding can be found here:
https://github.com/aws-samples/aws-glue-samples



Then start History Server
https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-history.html

Use the us-east-1 CF
https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?templateURL=https://aws-glue-sparkui-prod-us-east-1.s3.amazonaws.com/public/cfn/sparkui.yaml

Name: SparkUI
IP Address Range: 0.0.0.0/0
EventLogDir: s3a://lf-data-lake-bucket-565578166282/sparkui-history/
KeystorePassword: enter at least 6 character such as welcome1
VPC: choose the one called LF-Workshop-VPC
subnet: choose the one called LF-Workshop-PublicSubneOne


In outputs, grab the SparkUiPublicUrl
In a new browser window, open up that URL

